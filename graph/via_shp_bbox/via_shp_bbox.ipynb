{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "___________________________\n",
      "Part 3. Processing a graph (filtering and creating nodes)\n",
      "Please wait...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print()\n",
    "print(\"___________________________\")\n",
    "print(\"Part 3. Processing a graph (filtering and creating nodes)\")\n",
    "print(\"Please wait...\")\n",
    "print()\n",
    "\n",
    "\n",
    "import os\n",
    "import conda\n",
    "conda_file_dir = conda.__file__\n",
    "conda_dir = conda_file_dir.split('lib')[0]\n",
    "proj_lib = os.path.join(conda_dir, 'Library\\share')\n",
    "# proj_lib = os.path.join(os.path.join(conda_dir, 'pkgs'), 'proj4-5.2.0-h6538335_1006\\Library\\share')\n",
    "path_gdal = os.path.join(proj_lib, 'gdal')\n",
    "os.environ ['PROJ_LIB']=proj_lib\n",
    "os.environ ['GDAL_DATA']=path_gdal\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "\n",
    "#from tqdm.notebook import tqdm\n",
    "\n",
    "# # В случе ошибки RuntimeError: b'no arguments in initialization list'\n",
    "# # Если действие выше не помогло, то нужно задать системной переменной PROJ_LIB\n",
    "# # явный путь к окружению по аналогии ниже\n",
    "# Для настройки проекции координат, поменять на свой вариант\n",
    "\n",
    "\n",
    "# os.environ ['PROJ_LIB']=r'C:\\Users\\popova_kv\\AppData\\Local\\Continuum\\anaconda3\\Library\\share'\n",
    "# os.environ ['GDAL_DATA']=r'C:\\Users\\popova_kv\\AppData\\Local\\Continuum\\anaconda3\\Library\\share\\gdal'\n",
    "\n",
    "\n",
    "from shapely import wkt\n",
    "from shapely.wkt import loads\n",
    "from shapely.geometry import Point, LineString, MultiLineString, mapping, shape\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "import networkx as nx\n",
    "import momepy\n",
    "#import osmnx as ox\n",
    "import re\n",
    "import math\n",
    "\n",
    "#отключить предупреждения pandas (так быстрее считает!!!):\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from time import sleep\n",
    "pause = 0.1\n",
    "sleep(pause)\n",
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf_lines = gpd.GeoDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################\n",
    "# import gdf_from_osm\n",
    "# #############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del gdf_from_osm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_gdf_lines = gdf_from_osm.gdf_lines\n",
    "small_gdf_poly = gdf_from_osm.gdf_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_date = gdf_from_osm.str_date\n",
    "place = gdf_from_osm.place\n",
    "buff_km = gdf_from_osm.buff_km\n",
    "poly_osmid = gdf_from_osm.poly_osmid\n",
    "gdf_poly = gdf_from_osm.gdf_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flnm = './data/gdf_lines_{}_{}_{}.json'.format(buff_km, place, str_date)\n",
    "small_gdf_lines.to_file(flnm, driver=\"GeoJSON\", encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_lines = gpd.GeoDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_files = [\".\\\\data\\\\gdf_lines_0_Самара_20200721_1449.json\",\n",
    "\".\\\\data\\\\gdf_lines_0_Тольятти_20200721_1447.json\",\n",
    "\".\\\\data\\\\gdf_lines_0_Новокуйбышевск_20200721_1443.json\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "for i in range(len(lst_files)):\n",
    "    jsname = lst_files[i]\n",
    "    small_gdf_lines = gpd.read_file(jsname, encoding='utf-8')\n",
    "    gdf_lines = gdf_lines.append(small_gdf_lines).reset_index(drop=True)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_lines = gdf_lines.drop_duplicates(subset=['geometry']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_lines.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf_lines = gdf_lines[['osm_id', 'name', 'highway', 'z_order', 'other_tags', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flnm = './data/gdf_lines2_{}_{}_{}.json'.format(buff_km, \"all\", str_date)\n",
    "gdf_lines.to_file(flnm, driver=\"GeoJSON\", encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "buff_km=0\n",
    "place='СНТ'\n",
    "str_date='20200721_1449'\n",
    "poly_osmid='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_lines = gpd.read_file(\"./data/gdf_lines_0_all_20200721_1449.json\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_poly = gpd.read_file(\"./buffer/outer_border_SA.shp\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "len_gdf = len(gdf_lines)\n",
    "pause = round(0.000007 * len_gdf, 1)\n",
    "\n",
    "###############\n",
    "#path_data = './data/'\n",
    "#path_res = './data/'\n",
    "\n",
    "path_data = '.\\\\data\\\\' + str(poly_osmid) + '\\\\' + str_date\n",
    "path_res = path_data + '\\\\res'\n",
    "\n",
    "path_res_edges = path_res\n",
    "path_res_nodes = path_res\n",
    "####################\n",
    "\n",
    "###############\n",
    "len_elem = len(gdf_lines)\n",
    "time_min = int(len_elem / 125 / 60)\n",
    "time_max = int(len_elem / 55 / 60)\n",
    "\n",
    "\n",
    "###############\n",
    "sleep(pause)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst_ot_ped_bad = ['proposed', 'ferry', 'leisure', 'admin_level', 'wokrset', \n",
    "#                   'attraction', 'planned', 'building', 'leaf_type', 'power', \n",
    "#                   'abandoned', 'aeroway', 'ice_road', 'access\"=>\"no']\n",
    "# #\n",
    "# lst_hw_ped_bad = ['proposed', 'planned']\n",
    "\n",
    "# pedestrian = gdf_lines[((~gdf_lines.osm_id.isin(city_graph.osm_id)) \n",
    "#                         & (~gdf_lines['other_tags'].str.contains('|'.join(lst_ot_ped_bad), \n",
    "#                                                                  na=False)) \n",
    "#                         & (~gdf_lines.highway.isin(lst_hw_ped_bad)) \n",
    "#                         & (~gdf_lines['other_tags'].str.contains('=>\"rail\"', na=False))\n",
    "#                        )]\n",
    "# #\n",
    "# print(len(pedestrian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "# https://automating-gis-processes.github.io/site/notebooks/L3/spatial_index.html\n",
    "\n",
    "def intersect_using_spatial_index(source_gdf, intersecting_gdf):\n",
    "    \"\"\"\n",
    "    Conduct spatial intersection using spatial index for candidates GeoDataFrame to make queries faster.\n",
    "    Note, with this function, you can have multiple Polygons in the 'intersecting_gdf' and it will return all the points\n",
    "    intersect with ANY of those geometries.\n",
    "    \"\"\"\n",
    "    source_sindex = source_gdf.sindex\n",
    "    possible_matches_index = []\n",
    "\n",
    "    # 'itertuples()' function is a faster version of 'iterrows()'\n",
    "    for other in intersecting_gdf.itertuples():\n",
    "        bounds = other.geometry.bounds\n",
    "        c = list(source_sindex.intersection(bounds))\n",
    "        possible_matches_index += c\n",
    "\n",
    "    # Get unique candidates\n",
    "    unique_candidate_matches = list(set(possible_matches_index))\n",
    "    possible_matches = source_gdf.iloc[unique_candidate_matches]\n",
    "\n",
    "    # Conduct the actual intersect\n",
    "    result = possible_matches.loc[possible_matches.intersects(intersecting_gdf.unary_union)]\n",
    "    return result\n",
    "    ########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sjoin_borders(gdf_lines, gdf_poly):\n",
    "    \n",
    "        # оставить только те ребра, которые внутри полигона\n",
    "    try:\n",
    "#         print(\"sjoin start\")\n",
    "        gdf_lines_tmp = intersect_using_spatial_index(gdf_lines, gdf_poly[['geometry']])\n",
    "#         print(\"sjoin end\")\n",
    "        gdf_lines_tmp = gdf_lines_tmp.reset_index(drop=True)\n",
    "#         gdf_lines_tmp = gpd.sjoin(gdf_lines, gdf_poly[['geometry']], how='inner', \n",
    "#                               op='intersects').drop(\"index_right\", \n",
    "#                                                     axis=1).reset_index(drop=True)\n",
    "        #\n",
    "        if len(gdf_lines_tmp) > (len(gdf_lines) / 2):\n",
    "            gdf_lines = gdf_lines_tmp.copy()\n",
    "        gdf_lines_tmp = None\n",
    "        #gdf_poly = None\n",
    "        del gdf_lines_tmp#, gdf_poly\n",
    "    except:\n",
    "        pass\n",
    "    #\n",
    "    return gdf_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(gdf_lines, gdf_poly):\n",
    "\n",
    "    # file_name = 'petr_kamch'\n",
    "    # read_shp = gpd.read_file(r'./shp/raw/{}.shp'.format(file_name), encoding = 'utf-8', errors='ignore')\n",
    "    sleep(pause)\n",
    "\n",
    "\n",
    "    # удаление кривых символов в строке (которые не преобразовались по unicode)\n",
    "    # city_graph = read_shp.copy()\n",
    "    city_graph = gdf_lines.copy()\n",
    "    \n",
    "    sleep(pause)\n",
    "    #################\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #list_columns = [city_graph['name'], city_graph['other_tags']]\n",
    "    #np_city_gr = city_graph.to_numpy()\n",
    "\n",
    "    def bytesDecode(list_columns):\n",
    "        list_new_columns = []\n",
    "        for column in (list_columns):\n",
    "            list_strings = []\n",
    "            for row in (column):\n",
    "                new_row = row\n",
    "                if (isinstance(bytes(), type(row)) == True):\n",
    "                    list_new_values = []\n",
    "                    j=0\n",
    "                    new_value = \"\"\n",
    "                    for j in range(len(row)):\n",
    "                        try:\n",
    "                            new_value = row[j:j+1].decode() \n",
    "                            # декодирование по одному байтовому символу\n",
    "                        except (UnicodeDecodeError, AttributeError):\n",
    "                            try:\n",
    "                                new_value = row[j:j+2].decode() \n",
    "                                # у кириллицы на одну букву два байтовых символа\n",
    "                            except (UnicodeDecodeError, AttributeError):\n",
    "                                new_value = \"\"\n",
    "                        list_new_values.append(new_value)\n",
    "                    new_string = \"\"\n",
    "                    i=0\n",
    "                    for i in list_new_values:\n",
    "                        new_string = new_string+i\n",
    "                    #if (len(new_string.encode('utf-8')) >= 254): \n",
    "                        # максимальная длина строки в shp - 255\n",
    "                        #new_string = new_string[:254]\n",
    "                    new_row = new_string + '\"' #\n",
    "                list_strings.append(new_row)\n",
    "        #\n",
    "            list_new_columns.append(list_strings)\n",
    "        return list_new_columns\n",
    "        # \n",
    "    # \n",
    "    # ind_name = list(city_graph.columns).index('name')\n",
    "    # ind_ot = list(city_graph.columns).index('other_tags')\n",
    "    list_columns = [city_graph['name'], city_graph['other_tags']]\n",
    "    #list_columns = list(np_city_gr[:,ind_name], np_city_gr[:,ind_ot])\n",
    "\n",
    "    list_new_columns = bytesDecode(list_columns)\n",
    "    sleep(pause)\n",
    "\n",
    "\n",
    "    #################\n",
    "    city_graph['name'] = list_new_columns[0]\n",
    "    city_graph['other_tags'] = list_new_columns[1]\n",
    "\n",
    "\n",
    "    # до фильтрации сохранить нужные ребра трамвайных и жд путей\n",
    "    rail_tram = city_graph[(city_graph['other_tags'].str.contains('\"railway\"=>\"tram\"', na=False))]\n",
    "    rail_main = city_graph[(city_graph['other_tags'].str.contains('\"railway\"=>\"rail\"', na=False))]\n",
    "    rail_main = rail_main[((rail_main['other_tags'].str.contains('\"usage\"=>\"main\"', na=False)) \n",
    "                           | (rail_main['other_tags'].str.contains('\"usage\"=>\"branch\"', na=False))\n",
    "                           | ((~rail_main['other_tags'].str.contains('service\"=>\"', na=False)) \n",
    "                              & ((~rail_main['other_tags'].str.contains('usage\"=>\"', na=False)))))]\n",
    "    #\n",
    "    rail_subw = city_graph[(city_graph['other_tags'].str.contains('\"railway\"=>\"subway\"', na=False))]\n",
    "    rail_subw = rail_subw[~(rail_subw['other_tags'].str.contains('service', na=False))]\n",
    "\n",
    "\n",
    "    # удаление строк, содержаших ненужные значения\n",
    "    #  списки ненужных значений, которые надо будет удалить\n",
    "    lst_highway_notok = ['steps', 'pedestrian', 'footway', 'path', 'raceway', 'road', 'track', 'planned', 'proposed', 'cycleway']\n",
    "\n",
    "    lst_ot_notok = ['access\"=>\"no','abandoned','admin_level','aeroway','attraction','building',\n",
    "                    'ferry','grass','hiking', 'ice_road','land','leaf_type',\n",
    "                    'leisure','mud','natural','piste',\n",
    "                    'planned','power','private','proposed','wood','wokrset']\n",
    "    # some are ok: unpaved,description\n",
    "\n",
    "\n",
    "    city_graph = city_graph[\n",
    "        (city_graph.waterway.isna())\n",
    "        & (city_graph.aerialway.isna())\n",
    "        & (city_graph.man_made.isna())\n",
    "        & ((city_graph.barrier.isna()) | (city_graph['barrier'] == 'yes'))\n",
    "        & (~city_graph.highway.isin(lst_highway_notok))\n",
    "        & (~city_graph['other_tags'].str.contains('|'.join(lst_ot_notok), na=False))\n",
    "        & (~(city_graph['other_tags'].str.contains(\"sand\", na=False) & city_graph.name.isna()))\n",
    "        & (~((city_graph.z_order == 0) & (city_graph.name.isna())))\n",
    "        & (~((city_graph.highway == 'construction') \n",
    "              & (city_graph.other_tags.isna()) & (city_graph.name.isna())))\n",
    "        & (~((city_graph.highway == 'service') & (city_graph.name.isna())))\n",
    "                                 ].reset_index(drop=True)\n",
    "    #\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################\n",
    "    # пешеходные отдельно - добавление ниже, после раздеделния ребер на перекрестках(u_u)\n",
    "    lst_ot_ped_bad = ['proposed', 'ferry', 'leisure', 'admin_level', 'wokrset', \n",
    "                  'attraction', 'planned', 'building', 'leaf_type', 'power', \n",
    "                  'abandoned', 'aeroway', 'ice_road', 'access\"=>\"no']\n",
    "    #\n",
    "    lst_hw_ped_bad = ['proposed', 'planned']\n",
    "\n",
    "    pedestrian = gdf_lines[((~gdf_lines.osm_id.isin(city_graph.osm_id)) \n",
    "                            & (~gdf_lines['other_tags'].str.contains('|'.join(lst_ot_ped_bad), \n",
    "                                                                     na=False)) \n",
    "                            & (~gdf_lines.highway.isin(lst_hw_ped_bad))\n",
    "                            & (~gdf_lines['other_tags'].str.contains('railway', na=False))\n",
    "                           )]\n",
    "    #\n",
    "    ####################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    sleep(pause)\n",
    "    \n",
    "    #############################\n",
    "    # constructions - temporary changes in the road \n",
    "    # no name and no tags - new road, should be deleted\n",
    "\n",
    "    constr = city_graph[(((city_graph.highway == 'construction') \n",
    "                              & ~(city_graph.other_tags.isna()) \n",
    "                              & ~(city_graph.name.isna())) \n",
    "                             | ((city_graph.other_tags.str.contains(\"bridge\", na=False)) \n",
    "                                & (~(city_graph.name.isna()))\n",
    "                                & ((city_graph.highway.isna()))))].reset_index(drop=True)\n",
    "    #\n",
    "\n",
    "    #########\n",
    "    #city_graph2 = city_graph.copy()\n",
    "    cg_crs = city_graph.crs\n",
    "    np_cg2 = city_graph.to_numpy()\n",
    "    ind_hw = list(city_graph.columns).index('highway')\n",
    "    ind_oi = list(city_graph.columns).index('osm_id')\n",
    "\n",
    "    lst_contstr_name=list(constr.name.unique())\n",
    "    i=0\n",
    "    for i in (range(len(lst_contstr_name))):\n",
    "        one_name = lst_contstr_name[i]\n",
    "        df_small = constr[constr.name == one_name].reset_index(drop=True)\n",
    "        sj_df = intersect_using_spatial_index(city_graph,df_small)\n",
    "    #\t sj_df = gpd.sjoin(df_small, city_graph[['highway', 'name', 'geometry']], \n",
    "    #\t\t\t\t\t   how='inner', op='intersects').drop(\"index_right\", axis=1)\n",
    "        lst_hw = list(sj_df[((sj_df.name == one_name) \n",
    "                             & (sj_df.highway != 'construction') \n",
    "                             & ((sj_df.highway.astype(str) != 'None')))].highway.unique())\n",
    "        if lst_hw:\n",
    "            new_hw = lst_hw[0]\n",
    "        else:\n",
    "            new_hw = constr.highway[i]\n",
    "        for j in range(len(df_small)):\n",
    "            ind_big = list(np_cg2[:,ind_oi]).index(df_small.osm_id[j])\n",
    "            np_cg2[ind_big,ind_hw] = new_hw\n",
    "    # \n",
    "    sleep(pause)\n",
    "\n",
    "    lst_col = list(city_graph.columns)\n",
    "\n",
    "    city_graph = gpd.GeoDataFrame(np_cg2, columns=lst_col)\n",
    "    city_graph.crs = cg_crs\n",
    "    #########\n",
    "    #############################\n",
    "\n",
    "    # удаление ненужных и добавление нужных жд и трамвайных путей\n",
    "    city_graph = city_graph[\n",
    "        (~city_graph['other_tags'].str.contains('=>\"rail\"', na=False))\n",
    "        & (~city_graph['other_tags'].str.contains('railway', na=False))\n",
    "                         ]\n",
    "    city_graph = city_graph.append(rail_tram)\n",
    "    city_graph = city_graph.append(rail_main)\n",
    "    city_graph = city_graph.append(rail_subw)\n",
    "\n",
    "    #city_graph = city_graph[(~city_graph['other_tags'].str.contains('disused', na=False))]\n",
    "\n",
    "    city_graph = city_graph.reset_index(drop=True)\n",
    "\n",
    "    sleep(pause)\n",
    "\n",
    "    # select lines which are used in routes of public transport\n",
    "    try:\n",
    "        buff_gdf_multilines = gdf_multilines.to_crs('epsg:32637').buffer(0.5).to_crs('epsg:4326')\n",
    "        buff_gdf_multilines = gpd.GeoDataFrame(geometry=buff_gdf_multilines)\n",
    "        inter_gdf_lines =  gpd.sjoin(gdf_lines, buff_gdf_multilines, how='inner', \n",
    "                                     op='within').drop(\"index_right\", axis=1).reset_index(drop=True)\n",
    "\n",
    "        add_new_tmp = inter_gdf_lines[~inter_gdf_lines.osm_id.isin(list(city_graph.osm_id))] #add only new lines\n",
    "\n",
    "        add_new = gdf_lines[gdf_lines.osm_id.isin(list(add_new_tmp.osm_id))]\n",
    "        add_new = add_new[\n",
    "            (add_new.waterway.isna())\n",
    "            & (add_new.aerialway.isna())\n",
    "            & (add_new.barrier.isna())\n",
    "            & (add_new.man_made.isna()) \n",
    "            & ~add_new.highway.isin(lst_highway_notok) \n",
    "            & (~add_new['other_tags'].str.contains('|'.join(lst_ot_notok), \n",
    "                                                   na=False))].drop_duplicates()\n",
    "    #\n",
    "        city_graph = city_graph.append(add_new).reset_index(drop=True)\n",
    "    except:\n",
    "        pass\n",
    "    #\n",
    "    #####\n",
    "    #! ATTENTION!!!\n",
    "    gdf_lines = None\n",
    "    del gdf_lines\n",
    "    sleep(pause)\n",
    "    #####\n",
    "    ########################\n",
    "    # изменение направления ребра для oneway=-1\n",
    "    reverse_oneway = city_graph[city_graph.other_tags.str.contains('oneway\"=>\"-1', \n",
    "                                                                   na=False)].reset_index(drop=True)\n",
    "    #\n",
    "    ####################\n",
    "    np_city_gr = city_graph.to_numpy()\n",
    "    lst_rev_on = list(reverse_oneway.osm_id)\n",
    "    ind_geo = list(city_graph.columns).index('geometry')\n",
    "    ind_oi = list(city_graph.columns).index('osm_id')\n",
    "\n",
    "    def RevOnwGeo(np_city_gr,lst_rev_on,ind_geo,ind_oi):\n",
    "        lst_geo_new=[]\n",
    "        i=0\n",
    "        for i in range(len(np_city_gr)):\n",
    "            if np_city_gr[i][ind_oi] in lst_rev_on:\n",
    "                list_geo = list(np_city_gr[i][ind_geo].coords[:])\n",
    "                one_reversed_geo = list_geo[::-1]\n",
    "                line_2 = LineString(one_reversed_geo)\n",
    "                lst_geo_new.append(line_2)\n",
    "            else:\n",
    "                lst_geo_new.append(np_city_gr[i][ind_geo])\n",
    "        #\n",
    "        return lst_geo_new\n",
    "    # \n",
    "\n",
    "    lst_geo_new = RevOnwGeo(np_city_gr,lst_rev_on,ind_geo,ind_oi)\n",
    "    #################### \n",
    "    sleep(pause)\n",
    "\n",
    "    try:\n",
    "        city_graph['geometry'] = lst_geo_new\n",
    "    except:\n",
    "        print(\"Error_rev\")\n",
    "    # \n",
    "    ########################\n",
    "    \n",
    "    \n",
    "    city_graph = city_graph.append(pedestrian).reset_index(drop=True)\n",
    "    \n",
    "    print(\"uu_start\")\n",
    "    time_end = \"{:%H:%M:%S}\".format(datetime.now())\n",
    "    print(\"time start:\", time_end)\n",
    "\n",
    "    # #Обработка графа - дробление ребер по перекресткам и создание узлов (nodes)\n",
    "\n",
    "    # здесь происходит дробление ребер по всем пересечениям (даже на многоуровневых эстакадах)\n",
    "    # обработка эстакад будет ниже\n",
    "    lines = list(city_graph.geometry)\n",
    "    graph = unary_union(lines)\n",
    "    res_graph = gpd.GeoDataFrame(graph) \n",
    "    # если сделать через geometry=[graph] он делает из графа один большой multilinestring\n",
    "    res_graph = res_graph.rename(columns={0:'geometry'})\n",
    "    res_graph.crs='epsg:4326'\n",
    "    #res_graph = res_graph.to_crs('epsg:4326')\n",
    "    res_graph = res_graph.reset_index(drop=True)\n",
    "    res_graph = res_graph.reset_index()\n",
    "    \n",
    "    print(\"uu_end\")\n",
    "    time_end = \"{:%H:%M:%S}\".format(datetime.now())\n",
    "    print(\"time end:\", time_end)\n",
    "    sleep(pause)\n",
    "\n",
    "    # подтягивание полей с информацией по пересечению геометрий\n",
    "    graph_info = gpd.sjoin(res_graph, city_graph, how='left', \n",
    "                               op='within').drop(\"index_right\", axis=1).reset_index(drop=True)\n",
    "    #\n",
    "    del graph_info['index']\n",
    "    \n",
    "    ##############\n",
    "    \n",
    "    \n",
    "    \n",
    "    #graph_info = graph_info.append(pedestrian).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##############\n",
    "    \n",
    "    #################################\n",
    "    nans_g = graph_info[graph_info.osm_id.isna()]\n",
    "    if len (nans_g) != 0:\n",
    "        nans_g = gpd.sjoin(nans_g[['geometry']], city_graph, how='inner', \n",
    "                                   op='intersects').drop(\"index_right\", axis=1).reset_index(drop=True)\n",
    "    #\n",
    "    sleep(pause)\n",
    "\n",
    "    # эти необходимо пере_разбить\n",
    "    tmp_city = city_graph[city_graph.osm_id.isin(nans_g.osm_id)].reset_index(drop=True)\n",
    "\n",
    "    # удалить пустые и те, которые необходимо пере_разбить, остальные - ок\n",
    "    good_graph_info = graph_info[((~graph_info.osm_id.isna()) \n",
    "                                  & (~graph_info.osm_id.isin(tmp_city.osm_id)))].reset_index(drop=True)\n",
    "    #\n",
    "    ##############\n",
    "    np_tc = tmp_city.to_numpy()\n",
    "    ind_geo = list(tmp_city.columns).index('geometry')\n",
    "    ind_oi = list(tmp_city.columns).index('osm_id')\n",
    "\n",
    "    sleep(pause)\n",
    "    ########\n",
    "\n",
    "\n",
    "    lst_uu_geo = []\n",
    "    newlst=[]\n",
    "    i=0\n",
    "    for i in (range(len(np_tc))):\n",
    "        one_geo = gpd.GeoDataFrame(geometry=[np_tc[i,-1]])\n",
    "        one_geo.crs = city_graph.crs\n",
    "        tmp_sj = intersect_using_spatial_index(city_graph,one_geo)\n",
    "    #\t tmp_sj = gpd.sjoin(one_geo, city_graph, how='inner', \n",
    "    #\t\t\t\t\t\top='intersects').drop(\"index_right\", axis=1).reset_index(drop=True)\n",
    "        sj_one = list(city_graph[city_graph.osm_id.isin(tmp_sj.osm_id)].geometry)\n",
    "        lst_one_geo = np_tc[i][ind_geo].coords[:]\n",
    "        uniqlines = []\n",
    "        lst_sj_ends=[]\n",
    "        j=0\n",
    "        for j in range(len(sj_one)):\n",
    "            lst_uu = []\n",
    "            tmp_lst = []\n",
    "            tmp_lst.append(sj_one[j])\n",
    "            lst_sj_one_geo = sj_one[j].coords[:]\n",
    "            res = list(set(lst_one_geo) & set(lst_sj_one_geo)) #find mutual points\n",
    "            if len(res) > 0:\n",
    "                for k in res:\n",
    "                    if ((k != np_tc[i][ind_geo].coords[0]) & (k != np_tc[i][ind_geo].coords[-1])):\n",
    "                        if sj_one[j] not in lst_uu:\n",
    "                            lst_uu.append(sj_one[j])\n",
    "                    if ((k == sj_one[j].coords[0]) | (k == sj_one[j].coords[-1])):\n",
    "                        if sj_one[j] not in lst_sj_ends:\n",
    "                            lst_sj_ends.append(sj_one[j])\n",
    "            #\n",
    "            for line in lst_uu:\n",
    "                if not any(p.equals(line) for p in uniqlines):\n",
    "                    uniqlines.append(line)\n",
    "        if np_tc[i][ind_geo] not in uniqlines:\n",
    "                uniqlines.append(np_tc[i][ind_geo])\n",
    "        if len(uniqlines) > 1:\n",
    "            uu_geo = unary_union(uniqlines)\n",
    "            one_gdf = gpd.GeoDataFrame(geometry=list(uu_geo))\n",
    "            one_gdf.crs=city_graph.crs\n",
    "            tmp_one_gdf = gpd.sjoin(one_gdf, city_graph, how='left', \n",
    "                                    op='within').drop(\"index_right\", axis=1)\n",
    "            if len(tmp_one_gdf[tmp_one_gdf.osm_id.isna()]) > 0:\n",
    "                line_f = np_tc[i][ind_geo]\n",
    "                line = np_tc[i][ind_geo]\n",
    "                d=0\n",
    "                cnt=0\n",
    "                for d in range(len(lst_sj_ends)):\n",
    "                    try:\n",
    "                        point = Point(list(set(line_f.coords[:]) & set(lst_sj_ends[d].coords[:]))[0])\n",
    "                        if (((point.coords[0] != line_f.coords[0]) & (point.coords[0] != line_f.coords[-1])) \n",
    "                            & ((point.coords[0] == lst_sj_ends[d].coords[0]) | (point.coords[0] == lst_sj_ends[d].coords[-1]))):\n",
    "                            new_geo = MultiLineString(list(shapely.ops.split(line,point)))\n",
    "                            line = new_geo\n",
    "                            cnt+=1\n",
    "                    except:\n",
    "                        pass\n",
    "                if cnt > 0:\n",
    "                    newlst.append(new_geo)\n",
    "                else:\n",
    "                    new_geo = np_tc[i][ind_geo]\n",
    "            else:\n",
    "                new_geo = MultiLineString(list(tmp_one_gdf[tmp_one_gdf.osm_id \n",
    "                                                                        == np_tc[i][ind_oi]].geometry))\n",
    "        else:\n",
    "            new_geo = np_tc[i][ind_geo]\n",
    "        #\n",
    "        lst_uu_geo.append(new_geo)\n",
    "    # \n",
    "    ####\n",
    "    # ! ATTENTION\n",
    "    city_graph = None\n",
    "    del city_graph\n",
    "    ####\n",
    "\n",
    "    sleep(pause)\n",
    "    ##############\n",
    "    #\n",
    "    try:\n",
    "        tmp_city['uu_geo'] = lst_uu_geo\n",
    "    except:\n",
    "        print(\"Error_uu\")\n",
    "    #\n",
    "    ###############\n",
    "    np_tmp_ct = tmp_city.to_numpy()\n",
    "    ind_uug = list(tmp_city.columns).index('uu_geo')\n",
    "\n",
    "    sleep(pause)\n",
    "\n",
    "    new_df = []\n",
    "    i=0\n",
    "    for i in (range(len(np_tmp_ct))):\n",
    "\n",
    "        one_line = np_tmp_ct[i][ind_uug]\n",
    "        try:\n",
    "            len_ol = len(one_line)\n",
    "            j = 0\n",
    "            for j in range(len_ol):\n",
    "                lst_one = list(np_tmp_ct[i][:ind_uug])\n",
    "                lst_one.append(one_line[j])\n",
    "                new_df.append(lst_one)\n",
    "        except:\n",
    "            new_df.append(list(np_tmp_ct[i]))\n",
    "    # \n",
    "\n",
    "    sleep(pause)\n",
    "\n",
    "    ###############\n",
    "    new_gdf = gpd.GeoDataFrame(columns=tmp_city.columns, data=new_df)\n",
    "    del new_gdf['geometry']\n",
    "    new_gdf = new_gdf.rename(columns={'uu_geo':'geometry'})\n",
    "    new_gdf.crs='epsg:4326'\n",
    "\n",
    "    tr_gi = good_graph_info.append(new_gdf).reset_index(drop=True)\n",
    "    graph_filtrd = tr_gi.copy()\n",
    "    graph_filtrd['z_order'] = graph_filtrd['z_order'].astype(np.int64)\n",
    "    #################################\n",
    "\n",
    "    sleep(pause)\n",
    "\n",
    "    #################################################\n",
    "    # создание digraph (двунаправленного графа)\n",
    "\n",
    "    ###### Create Reverse of graph\n",
    "    #direct_gdf = new_graph[['osm_id', 'name', 'highway', 'z_order', 'other_tags', 'geometry']].copy()\n",
    "\n",
    "    direct_gdf = graph_filtrd[['osm_id', 'name', 'highway', 'z_order', 'other_tags', 'geometry']].copy()\n",
    "    reverse_gdf = direct_gdf.copy()\n",
    "\n",
    "    sleep(pause)\n",
    "\n",
    "    ###############\n",
    "    np_rev_gdf = reverse_gdf.to_numpy()\n",
    "    ind_geo = list(reverse_gdf.columns).index('geometry')\n",
    "    reversed_geo_all = []\n",
    "    i=0\n",
    "    for i in (range(len(np_rev_gdf))):\n",
    "        list_geo = list(np_rev_gdf[i][ind_geo].coords[:])\n",
    "        one_reversed_geo = list_geo[::-1]\n",
    "        line_2 = LineString(one_reversed_geo)\n",
    "        reversed_geo_all.append(line_2)\n",
    "    # \n",
    "    ###############\n",
    "\n",
    "    #rev_geo = gpd.GeoDataFrame(geometry=reversed_geo_all)\n",
    "    try:\n",
    "        reverse_gdf['rev_geo'] = reversed_geo_all\n",
    "        reverse_gdf = reverse_gdf.rename(columns={'geometry':'old_geo', 'rev_geo':'geometry'})\n",
    "        reverse_gdf = reverse_gdf[['osm_id', 'name', 'highway', 'z_order', 'other_tags', 'geometry']]\n",
    "    except:\n",
    "        print(\"Error!\")\n",
    "    #\n",
    "    sleep(pause)\n",
    "\n",
    "\n",
    "    direct_gdf['direction'] = \"direct\"\n",
    "    reverse_gdf['direction'] = \"reverse\"\n",
    "    all_gdf = direct_gdf.append(reverse_gdf).reset_index(drop=True)\n",
    "    all_gdf.crs = 'epsg:4326'\n",
    "\n",
    "\n",
    "    ############################################\n",
    "\n",
    "    # разбиение петель и ребер, где на пару node_from node_to - больше 2 ребер\n",
    "\n",
    "    tmp_grph = all_gdf.copy()\n",
    "    #\n",
    "    lst_petl = []\n",
    "    lst_start = []\n",
    "    lst_end = []\n",
    "    lst_stend = []\n",
    "\n",
    "    #############\n",
    "    np_tmp_gr = tmp_grph.to_numpy()\n",
    "    ind_geo = list(tmp_grph.columns).index('geometry')\n",
    "\n",
    "    sleep(pause)\n",
    "\n",
    "    i=0\n",
    "    for i in (range(len(np_tmp_gr))):\n",
    "        if np_tmp_gr[i][ind_geo].coords[0] == np_tmp_gr[i][ind_geo].coords[-1]:\n",
    "            lst_petl.append(np_tmp_gr[i][ind_geo])\n",
    "        else:\n",
    "            lst_stend.append(str(np_tmp_gr[i][ind_geo].coords[0]) + \"_\" + str(np_tmp_gr[i][ind_geo].coords[-1]))\n",
    "    # \n",
    "    #############\n",
    "    sleep(pause)\n",
    "\n",
    "    my_dict = {i:lst_stend.count(i) for i in lst_stend}\n",
    "\n",
    "    newDict = {}\n",
    "    for (key, value) in my_dict.items():\n",
    "        if value > 1:\n",
    "            newDict[key] = value\n",
    "    #\n",
    "\n",
    "    #\n",
    "    lst_geo = []\n",
    "    lst_len_geo = []\n",
    "    ###########\n",
    "    # np_tmp_gr = tmp_grph.to_numpy()\n",
    "    # ind_geo = list(tmp_grph.columns).index('geometry')\n",
    "    i=0\n",
    "    for i in range(len(np_tmp_gr)):\n",
    "        str_st_end = str(np_tmp_gr[i][ind_geo].coords[0]) + \"_\" + str(np_tmp_gr[i][ind_geo].coords[-1])\n",
    "        if str_st_end in (newDict.keys()):\n",
    "            lst_geo.append(str_st_end)\n",
    "            lst_len_geo.append(np_tmp_gr[i][ind_geo].length)\n",
    "        else:\n",
    "            lst_geo.append(None)\n",
    "            lst_len_geo.append(None)\n",
    "    #\n",
    "    ###########\n",
    "\n",
    "    sleep(pause)\n",
    "\n",
    "    tmp_grph['geo_grup'] = lst_geo\n",
    "    tmp_grph['geo_len'] = lst_len_geo\n",
    "\n",
    "    ################\n",
    "    np_tmp_gr = tmp_grph.to_numpy()\n",
    "    ind_ggr = list(tmp_grph.columns).index('geo_grup')\n",
    "    ind_glen = list(tmp_grph.columns).index('geo_len')\n",
    "\n",
    "    dct_gr_len = {}\n",
    "    i=0\n",
    "    for i in range(len(np_tmp_gr)):\n",
    "        one_group = np_tmp_gr[i][ind_ggr]\n",
    "        if one_group not in dct_gr_len.keys():\n",
    "            lst_gr_len = []\n",
    "            dct_gr_len[one_group] = lst_gr_len\n",
    "        dct_gr_len[one_group] = dct_gr_len[one_group] + [np_tmp_gr[i][ind_glen]]  \n",
    "    # \n",
    "    ################\n",
    "    sleep(pause)\n",
    "\n",
    "    del dct_gr_len[None]\n",
    "    del tmp_grph['geo_len'], tmp_grph['geo_grup']\n",
    "\n",
    "    ################\n",
    "    np_tmp_gr = tmp_grph.to_numpy()\n",
    "    ind_geo = list(tmp_grph.columns).index('geometry')\n",
    "\n",
    "    lst_max = []\n",
    "    i=0\n",
    "    for i in range(len(np_tmp_gr)):\n",
    "        one_group = str(np_tmp_gr[i][ind_geo].coords[0]) + \"_\" + str(np_tmp_gr[i][ind_geo].coords[-1])\n",
    "        if one_group in dct_gr_len.keys():\n",
    "            if (max(dct_gr_len[one_group]) == np_tmp_gr[i][ind_geo].length):\n",
    "                lst_max.append(1)\n",
    "            else:\n",
    "                lst_max.append(None)\n",
    "        elif np_tmp_gr[i][ind_geo].coords[0] == np_tmp_gr[i][ind_geo].coords[-1]:\n",
    "            lst_max.append(1)\n",
    "        else:\n",
    "            lst_max.append(None)\n",
    "    # \n",
    "    ################\n",
    "\n",
    "    try:\n",
    "        tmp_grph['cut_geo'] = lst_max\n",
    "    except:\n",
    "        print(\"Error_cut_double\")\n",
    "    #\n",
    "    #########################\n",
    "\n",
    "    sleep(pause)\n",
    "\n",
    "    # функция обрезки ребер\n",
    "    def cut(line, distance):\n",
    "        # Cuts a line in two at a distance from its starting point\n",
    "        if distance <= 0.0 or distance >= line.length:\n",
    "            return [LineString(line)]\n",
    "        coords = list(line.coords)\n",
    "        for i, p in enumerate(coords):\n",
    "            pd = line.project(Point(p))\n",
    "            if pd == distance:\n",
    "                return [\n",
    "                    LineString(coords[:i+1]),\n",
    "                    LineString(coords[i:])]\n",
    "            if pd > distance:\n",
    "                cp = line.interpolate(distance)\n",
    "                return [\n",
    "                    LineString(coords[:i] + [(cp.x, cp.y)]),\n",
    "                    LineString([(cp.x, cp.y)] + coords[i:])]\n",
    "    # \n",
    "    #########################\n",
    "    all_ok = tmp_grph[tmp_grph.cut_geo != 1].reset_index(drop=True)\n",
    "    cut_gdf = tmp_grph[tmp_grph.cut_geo == 1].reset_index(drop=True)\n",
    "\n",
    "    ################\n",
    "    np_ctgdf = cut_gdf.to_numpy()\n",
    "    ind_geo = list(cut_gdf.columns).index('geometry')\n",
    "\n",
    "    big_lst = []\n",
    "    i=0\n",
    "    for i in (range(len(np_ctgdf))):\n",
    "        line = np_ctgdf[i][ind_geo]\n",
    "        lst_one_geo = cut(line, (line.length / 2))\n",
    "        one_list = list(np_ctgdf[i,:ind_geo]) + [lst_one_geo[0]] + list(np_ctgdf[i,ind_geo+1:])\n",
    "        two_list = list(np_ctgdf[i,:ind_geo]) + [lst_one_geo[1]] + list(np_ctgdf[i,ind_geo+1:])\n",
    "        big_lst.append(one_list)\n",
    "        big_lst.append(two_list)\n",
    "    # \n",
    "    sleep(pause)\n",
    "\n",
    "    big_gdf = gpd.GeoDataFrame(big_lst, columns=list(cut_gdf.columns))\n",
    "    ################ \n",
    "    big_gdf = all_ok.append(big_gdf).reset_index(drop=True)\n",
    "\n",
    "    del big_gdf['cut_geo']\n",
    "\n",
    "    big_gdf.crs = 'epsg:4326'\n",
    "    big_gdf = big_gdf.to_crs('epsg:32637')\n",
    "    #########################\n",
    "\n",
    "    sleep(pause)\n",
    "\n",
    "    ############################################\n",
    "\n",
    "    ##############\n",
    "\n",
    "    def make_graph_great_again(gdf):\n",
    "        G = momepy.gdf_to_nx(gdf, approach='primal')\n",
    "\n",
    "        # выбор наибольшего графа из подграфов \n",
    "        # (когда ребра удаляются выше, остаются подвешенные куски графа, их надо удалить)\n",
    "         # whatever graph you're working with\n",
    "        cur_graph = G\n",
    "    #\t def del_subgraphs(cur_graph):\n",
    "        list_subgraphs = [cur_graph]\n",
    "        if not nx.is_connected(cur_graph):\n",
    "            # get a list of unconnected networks\n",
    "            def connected_component_subgraphs(cur_graph):\n",
    "                for c in nx.connected_components(cur_graph):\n",
    "                    yield cur_graph.subgraph(c)\n",
    "            sub_graphs = connected_component_subgraphs(cur_graph)\n",
    "            list_graph = []\n",
    "            i=0\n",
    "            for i in sub_graphs:\n",
    "                list_graph.append(i)\n",
    "\n",
    "            main_graph = list_graph[0]\n",
    "            list_subgraphs = []\n",
    "            #list_subgraphs.append(main_graph)\n",
    "\n",
    "            # find the largest network in that list\n",
    "            for sg in list_graph:\n",
    "                if len(sg.nodes()) > len(main_graph.nodes()):\n",
    "                    main_graph = sg\n",
    "                else:\n",
    "                    list_subgraphs.append(sg)\n",
    "            try:\n",
    "                list_subgraphs.remove(main_graph)\n",
    "            except:\n",
    "                pass\n",
    "            cur_graph = main_graph\n",
    "            #\n",
    "        #####\n",
    "\n",
    "        #create gdfs\n",
    "        # формирование таблиц из графа и узлов (nodes)\n",
    "        nodes, new_graph = momepy.nx_to_gdf(cur_graph)\n",
    "\n",
    "        return nodes, new_graph\n",
    "\n",
    "    all_nodes, all_edges = make_graph_great_again(big_gdf)\n",
    "\n",
    "    sleep(pause)\n",
    "\n",
    "    #################################################\n",
    "\n",
    "    # all_graph = momepy.gdf_to_nx(big_gdf)\n",
    "    # all_nodes, all_edges = momepy.nx_to_gdf(all_graph)\n",
    "\n",
    "    all_edges.crs = 'epsg:32637'\n",
    "    all_edges = all_edges.to_crs('epsg:4326')\n",
    "    all_nodes.crs = 'epsg:32637'\n",
    "    all_nodes = all_nodes.to_crs('epsg:4326')\n",
    "\n",
    "\n",
    "\n",
    "    ########### Check node_from and node_to - change if wrong ###########\n",
    "\n",
    "    check_points = all_edges.copy()\n",
    "\n",
    "    check_points = check_points.rename(columns={'geometry': 'line_geometry'})\n",
    "\n",
    "    check_points['nodeID'] = check_points['node_start']\n",
    "    check_points = check_points.merge(all_nodes, how='left', on=['nodeID'])\n",
    "    check_points = check_points.rename(columns={'geometry': 'start_geometry'})\n",
    "\n",
    "    check_points['nodeID'] = check_points['node_end']\n",
    "    check_points = check_points.merge(all_nodes, how='left', on=['nodeID'])\n",
    "    check_points = check_points.rename(columns={'geometry': 'end_geometry'})\n",
    "\n",
    "    del check_points['nodeID']\n",
    "\n",
    "    ##################\n",
    "    np_cp = check_points.to_numpy()\n",
    "    ind_ne = list(check_points.columns).index('node_end')\n",
    "    ind_ns = list(check_points.columns).index('node_start')\n",
    "    ind_sg = list(check_points.columns).index('start_geometry')\n",
    "    ind_eg = list(check_points.columns).index('end_geometry')\n",
    "    ind_lg = list(check_points.columns).index('line_geometry')\n",
    "\n",
    "    # check_points['start_true'] = None\n",
    "    # check_points['end_true'] = None\n",
    "\n",
    "    sleep(pause)\n",
    "\n",
    "    list_check_start = []\n",
    "    list_check_end = []\n",
    "    i=0\n",
    "    for i in (range(len(np_cp))):\n",
    "        if np_cp[i][ind_sg].coords[0] == np_cp[i][ind_lg].coords[0]:\n",
    "            list_check_start.append(np_cp[i][ind_ns])\n",
    "        elif np_cp[i][ind_eg].coords[0] == np_cp[i][ind_lg].coords[0]:\n",
    "            list_check_start.append(np_cp[i][ind_ne])\n",
    "        else:\n",
    "            list_check_start.append(None)\n",
    "    # \n",
    "\n",
    "    for ii in (range(len(np_cp))):\n",
    "        if np_cp[ii][ind_eg].coords[0] == np_cp[ii][ind_lg].coords[-1]:\n",
    "            list_check_end.append(np_cp[ii][ind_ne])\n",
    "        elif np_cp[ii][ind_sg].coords[0] == np_cp[ii][ind_lg].coords[-1]:\n",
    "            list_check_end.append(np_cp[ii][ind_ns])\n",
    "        else:\n",
    "            list_check_end.append(None)\n",
    "    #\n",
    "    ##################\n",
    "\n",
    "    try:\n",
    "        check_points['start_true'] = list_check_start\n",
    "        check_points['end_true'] = list_check_end\n",
    "    except:\n",
    "        print(\"Error1\")\n",
    "    # \n",
    "\n",
    "    sleep(pause)\n",
    "\n",
    "    ########### delete oneways reverse #############\n",
    "\n",
    "    ok_oneway = check_points.copy()\n",
    "\n",
    "    ok_oneway = ok_oneway.reset_index(drop=True)\n",
    "    ok_oneway = ok_oneway.reset_index()\n",
    "    ok_oneway = ok_oneway.rename(columns={'index':'link_id'})\n",
    "    ok_oneway['link_id'] = ok_oneway['link_id'] + 1\n",
    "\n",
    "    ok_oneway = ok_oneway[['link_id', 'osm_id', 'name', 'highway', 'z_order', 'other_tags',\n",
    "                           'line_geometry', 'direction', 'mm_len', 'start_true', 'end_true']]\n",
    "    ok_oneway = ok_oneway.rename(columns={'line_geometry':'geometry', \n",
    "                                          'start_true':'node_start', \n",
    "                                          'end_true':'node_end'})\n",
    "    graph_full = ok_oneway.copy()\n",
    "    graph_full = gpd.GeoDataFrame(graph_full)\n",
    "    graph_full.crs = 'epsg:4326'\n",
    "\n",
    "    graph_full['z_order'] = graph_full['z_order'].astype(np.int64)\n",
    "    graph_full['mm_len'] = round((graph_full['mm_len'] / 1000), 3)\n",
    "\n",
    "    sleep(pause)\n",
    "\n",
    "    ############\n",
    "    # create num_lanes column\n",
    "    np_gf = graph_full.to_numpy()\n",
    "    ind_ot = list(graph_full.columns).index('other_tags')\n",
    "    ind_dir = list(graph_full.columns).index('direction')\n",
    "\n",
    "    list_lanes = []\n",
    "    reg = re.compile('[^0-9]')\n",
    "\n",
    "    sleep(pause)\n",
    "\n",
    "    lst_onw=['oneway\"=>\"yes','oneway\"=>\"1','oneway\"=>\"true', 'oneway\"=>\"-1']\n",
    "    i=0\n",
    "    for i in range(len(np_gf)):\n",
    "        str1 = str(np_gf[i][ind_ot])\n",
    "        if any((c in str1) for c in lst_onw):\n",
    "            if np_gf[i][ind_dir] == 'direct':\n",
    "                if '\"lanes\"=>\"' in str1:\n",
    "                    str2 = str1[str1.find('\"lanes\"=>\"') : ].split(\",\", 1)[0]\n",
    "                    int_lanes = int(reg.sub('', str2))\n",
    "                    list_lanes.append(int_lanes)\n",
    "                else:\n",
    "                    list_lanes.append(1)\n",
    "            else:\n",
    "                list_lanes.append(0)\n",
    "        else:\n",
    "            if '\"lanes\"=>\"' in str1:\n",
    "                str2 = str1[str1.find('\"lanes\"=>\"') : ].split(\",\", 1)[0]\n",
    "                int_lanes = int(reg.sub('', str2))\n",
    "                if int_lanes > 1:\n",
    "                    if np_gf[i][ind_dir] == 'direct':\n",
    "                        list_lanes.append(math.ceil(int_lanes/2))\n",
    "                    else:\n",
    "                        list_lanes.append(math.floor(int_lanes/2))\n",
    "                else:\n",
    "                    list_lanes.append(1)\n",
    "            else:\n",
    "                list_lanes.append(1)\n",
    "    # \n",
    "\n",
    "    sleep(pause)\n",
    "    ############\n",
    "\n",
    "    try:\n",
    "        graph_full['NUMLANES'] = list_lanes\n",
    "    except:\n",
    "        print(\"Error2\")\n",
    "    #  \n",
    "\n",
    "    #############\n",
    "    np_gf = graph_full.to_numpy()\n",
    "    ind_ot = list(graph_full.columns).index('other_tags')\n",
    "    lst_types = []\n",
    "\n",
    "    sleep(pause)\n",
    "\n",
    "    i=0\n",
    "    for i in (range(len(np_gf))):\n",
    "        if \"railway\" in str(np_gf[i][ind_ot]):\n",
    "            if '=>\"tram\"' in str(np_gf[i][ind_ot]):\n",
    "                if 'surface' in str(np_gf[i][ind_ot]):\n",
    "                    lst_types.append(\"TM,CAR,BUS,TB,MT\")\n",
    "                else:\n",
    "                    lst_types.append(\"TM\")\n",
    "            elif 'subway' in str(np_gf[i][ind_ot]):\n",
    "                lst_types.append(\"MTR\")\n",
    "            else:\n",
    "                lst_types.append(\"E\")\n",
    "        else:\n",
    "            if (\n",
    "                ('psv\"=>\"only\"' in str(np_gf[i][ind_ot]))\n",
    "                |\n",
    "                (('psv\"=>\"yes\"' in str(np_gf[i][ind_ot]))\n",
    "                    & ('vehicle\"=>\"no\"' in str(np_gf[i][ind_ot])))):\n",
    "                lst_types.append(\"BUS,TB,MT\")\n",
    "            else:\n",
    "                lst_types.append(\"CAR,BUS,TB,MT\")\n",
    "    #\n",
    "\n",
    "    sleep(pause)\n",
    "    #############\n",
    "\n",
    "    try:\n",
    "        graph_full['TSYSSET'] = lst_types\n",
    "    except:\n",
    "        print(\"Error3\")\n",
    "    # \n",
    "\n",
    "    ##############################\n",
    "    # add type link\n",
    "\n",
    "    ################\n",
    "    # add type link\n",
    "    np_gf = graph_full.to_numpy()\n",
    "    ind_ot = list(graph_full.columns).index('other_tags')\n",
    "    ind_hw = list(graph_full.columns).index('highway')\n",
    "    ind_nm = list(graph_full.columns).index('name')\n",
    "    ind_nl = list(graph_full.columns).index('NUMLANES')\n",
    "\n",
    "    sleep(pause)\n",
    "\n",
    "    lst_typeno = []\n",
    "    i=0\n",
    "    for i in range(len(np_gf)):\n",
    "        if \"railway\" in str(np_gf[i][ind_ot]):\n",
    "            if \"subway\" in str(np_gf[i][ind_ot]):\n",
    "                lst_typeno.append(10)\n",
    "            elif \"tram\" in str(np_gf[i][ind_ot]):\n",
    "                lst_typeno.append(40)\n",
    "            else:\n",
    "                lst_typeno.append(20)\n",
    "        elif np_gf[i][ind_nl] == 0:\n",
    "            lst_typeno.append(0)\n",
    "        else:\n",
    "            if np_gf[i][ind_hw] in ['motorway','trunk']:\n",
    "                lst_typeno.append(1)\n",
    "            elif np_gf[i][ind_hw] == 'primary':\n",
    "                lst_typeno.append(2)\n",
    "            elif np_gf[i][ind_hw] == 'secondary':\n",
    "                lst_typeno.append(3)\n",
    "            elif np_gf[i][ind_hw] == 'tertiary':\n",
    "                lst_typeno.append(4)\n",
    "            elif np_gf[i][ind_hw] in ['motorway_link','trunk_link', 'primary_link', 'secondary_link', 'tertiary_link']:\n",
    "                lst_typeno.append(5)\n",
    "            elif np_gf[i][ind_nm] != None:\n",
    "                lst_typeno.append(6)\n",
    "            else:\n",
    "                lst_typeno.append(7)\n",
    "    # \n",
    "\n",
    "    sleep(pause)\n",
    "    ################\n",
    "\n",
    "    try:\n",
    "        graph_full['TYPENO_2'] = lst_typeno\n",
    "    except:\n",
    "        print(\"Error_typeno\")\n",
    "\n",
    "    ##############################\n",
    "    \n",
    "    #############\n",
    "    # add maxspeed\n",
    "\n",
    "    np_gf = graph_full.to_numpy()\n",
    "    ind_ot = list(graph_full.columns).index('other_tags')\n",
    "\n",
    "    reg = re.compile('[^0-9]')\n",
    "\n",
    "    lst_mxsp = []\n",
    "    i=0\n",
    "    for i in range(len(np_gf)):\n",
    "        one_tag = np_gf[i,ind_ot]\n",
    "        if '\"maxspeed\"=>\"' in one_tag:\n",
    "            str2 = one_tag[one_tag.find('\"maxspeed\"=>\"') : ].split(\",\", 1)[0]\n",
    "            try:\n",
    "                int_spd = int(reg.sub('', str2))\n",
    "            except:\n",
    "                int_spd = 60\n",
    "        #\n",
    "\n",
    "        else:\n",
    "            int_spd = None\n",
    "        lst_mxsp.append(int_spd)\n",
    "    #\n",
    "    graph_full['maxspeed'] = lst_mxsp\n",
    "    \n",
    "    ind_oi = list(graph_full.columns).index('osm_id')\n",
    "    ind_tp = list(graph_full.columns).index('TSYSSET')\n",
    "    lst_oi_ped = list(pedestrian.osm_id)\n",
    "\n",
    "    lst_type_ped = []\n",
    "    i=0\n",
    "    for i in range(len(np_gf)):\n",
    "        osmid = np_gf[i,ind_oi]\n",
    "        tp_ts = np_gf[i,ind_tp]\n",
    "        if osmid in lst_oi_ped:\n",
    "            type_ped = 'only_ped'\n",
    "        elif tp_ts == 'E':\n",
    "            type_ped = 'no_ped'\n",
    "        else:\n",
    "            type_ped = 'car_n_ped'\n",
    "        #\n",
    "        lst_type_ped.append(type_ped)\n",
    "\n",
    "    graph_full['type_ped'] = lst_type_ped\n",
    "    \n",
    "\n",
    "    #############\n",
    "\n",
    "    graph_full.crs='epsg:4326'\n",
    "#     graph_full = graph_full.rename(columns={ 'mm_len':'link_len', \n",
    "#                                             'node_start':'from_node', 'node_end':'to_node'})\n",
    "    #\n",
    "    try:\n",
    "        graph_full = graph_full.rename(columns={'FROMNODENO':'from_node',\n",
    "                                                'TONODENO':'to_node',\n",
    "                                            'NUMLANES':'lanes',\n",
    "                                            'TSYSSET':'type_ts',\n",
    "                                            'TYPENO_2':'link_type',\n",
    "                                                'mm_len':'link_len',\n",
    "                                                'node_start':'from_node',\n",
    "                                                'node_end':'to_node'})\n",
    "        #\n",
    "    except:\n",
    "        pass\n",
    "    #\n",
    "    try:\n",
    "        graph_full = graph_full[['osm_id','link_id', 'name', \n",
    "                         'highway','z_order','type_ts', 'link_type',\n",
    "                         'lanes', 'maxspeed','direction','link_len',\n",
    "                                 'type_ped',\n",
    "                         'from_node', 'to_node', 'other_tags','geometry']]\n",
    "        #\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return graph_full, all_nodes\n",
    "    #\n",
    "######### End of graph checking #############\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time start: 18:23:11\n",
      "Estimated time: 22 to 39 minutes\n",
      "uu_start\n",
      "time start: 18:24:25\n",
      "uu_end\n",
      "time end: 18:29:23\n",
      "time end: 19:43:45\n"
     ]
    }
   ],
   "source": [
    "sleep(pause)\n",
    "\n",
    "\n",
    "len_elem = len(gdf_lines)\n",
    "time_min = int(len_elem / 45 / 60)\n",
    "time_max = int(len_elem / 25 / 60)\n",
    "\n",
    "time_start = \"{:%H:%M:%S}\".format(datetime.now())\n",
    "print(\"time start:\", time_start)\n",
    "print(\"Estimated time: {} to {} minutes\".format(time_min,time_max))\n",
    "\n",
    "gdf_lines_sj = get_sjoin_borders(gdf_lines, gdf_poly)\n",
    "\n",
    "graph_full,all_nodes = main(gdf_lines_sj, gdf_poly)\n",
    "\n",
    "sleep(pause)\n",
    "\n",
    "time_end = \"{:%H:%M:%S}\".format(datetime.now())\n",
    "print(\"time end:\", time_end)\n",
    "\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time start: 18:23:11\n",
    "# Estimated time: 22 to 39 minutes\n",
    "# uu_start\n",
    "# time start: 18:24:25\n",
    "# uu_end\n",
    "# time end: 18:29:23\n",
    "# time end: 19:43:45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flnm = './data/new_graph4_{}_{}_{}.json'.format(buff_km, \"all\", str_date)\n",
    "graph_full.to_file(flnm, driver=\"GeoJSON\", encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "flnm = './data/all_nodes4_{}_{}_{}.json'.format(buff_km, \"all\", str_date)\n",
    "all_nodes.to_file(flnm, driver=\"GeoJSON\", encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def saveMe(graph_full,all_nodes,str_date,place,buff_km,poly_osmid):\n",
    "\n",
    "    graph_full_shp = graph_full.copy()\n",
    "    np_gf = graph_full_shp.to_numpy()\n",
    "    ind_ot = list(graph_full_shp.columns).index('other_tags')\n",
    "\n",
    "    sleep(pause)\n",
    "\n",
    "    # обрезать для сохранения в шейп\n",
    "    lst_ot = []\n",
    "    i=0\n",
    "    for i in range(len(np_gf)):\n",
    "        if len(str(np_gf[i][ind_ot])) > 254:\n",
    "            lst_ot.append(np_gf[i][ind_ot][:254])\n",
    "        else:\n",
    "            lst_ot.append(np_gf[i][ind_ot])\n",
    "    # \n",
    "    graph_full_shp['other_tags'] = lst_ot\n",
    "\n",
    "    sleep(pause)\n",
    "\n",
    "    graph_full_shp.to_file('{}\\\\new_graph_{}_{}_{}.shp'.format(path_res_edges,buff_km, place, str_date), encoding='utf-8')\n",
    "    #graph_full_shp.to_file('./new_graph_{}_{}_{}.shp'.format(buff_km, place, str_date), encoding='utf-8')\n",
    "\n",
    "\n",
    "    all_nodes = all_nodes.rename(columns={'nodeID':'NO'})\n",
    "    all_nodes['XCOORD'] = all_nodes.geometry.x\n",
    "    all_nodes['YCOORD'] = all_nodes.geometry.y\n",
    "    all_nodes.to_file('{}\\\\nodes_{}_{}_{}.shp'.format(path_res_nodes,buff_km, place, str_date), encoding='utf-8')\n",
    "    #all_nodes.to_file('./all_nodes_{}_{}_{}.shp'.format(buff_km, place, str_date), encoding='utf-8')\n",
    "    #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sleep(pause)\n",
    "# saveMe(graph_full,all_nodes,str_date,place,buff_km,poly_osmid)\n",
    "\n",
    "# time_end = \"{:%H:%M:%S}\".format(datetime.now())\n",
    "# print(\"time end:\", time_end)\n",
    "# print(\"Results are in folder 'res'\")\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_full = graph_full.rename(columns={'NO':'link_id',\n",
    "                                        'LENGTH':'link_len',\n",
    "                                        'FROMNODENO':'from_node',\n",
    "                                        'TONODENO':'to_node',\n",
    "                                        'NUMLANES':'lanes',\n",
    "                                        'TSYSSET':'type_ts',\n",
    "                                        'TYPENO_2':'link_type'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_full = graph_full[['osm_id','link_id', 'name', \n",
    "                         'highway','z_order','type_ts', 'link_type',\n",
    "                         'lanes', 'maxspeed','direction','link_len',\n",
    "                         'from_node', 'to_node', 'other_tags','geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flnm = './data/new_graph2_{}_{}_{}.json'.format(buff_km, \"all\", str_date)\n",
    "graph_full.to_file(flnm, driver=\"GeoJSON\", encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
